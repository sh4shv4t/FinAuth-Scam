{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model Training for Credit Card Fraud Detection\n",
    "\n",
    "This notebook walks through the complete process of training an Artificial Neural Network (ANN) to detect fraudulent credit card transactions. We will use the popular [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) dataset from Kaggle.\n",
    "\n",
    "**The process is divided into five main steps:**\n",
    "1.  **Setup and Data Download**: We'll set up the Kaggle API and download the dataset.\n",
    "2.  **Data Preprocessing**: We'll load, explore, and prepare the data for our model.\n",
    "3.  **ANN Model Building**: We'll define the architecture of our neural network.\n",
    "4.  **Model Training and Evaluation**: We'll train the model on our data, handling the class imbalance, and then evaluate its performance.\n",
    "5.  **Saving the Model**: We'll save the final trained model for use in our backend application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Data Download\n",
    "\n",
    "First, we need to install the necessary libraries and set up the Kaggle API to download our dataset directly. \n",
    "\n",
    "**Important:** Before running the cell below, you must upload your `kaggle.json` API token file. You can do this by clicking the 'folder' icon on the left sidebar in Google Colab and uploading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\Shashvat Singh/.kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Set up Kaggle directory and permissions\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(\u001b[33m'\u001b[39m\u001b[33m~/.kaggle\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m~/.kaggle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Make sure kaggle.json is in the right place\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# If running locally, ensure ~/.kaggle/kaggle.json exists.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# If in Colab, upload kaggle.json to the session storage.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(\u001b[33m'\u001b[39m\u001b[33mkaggle.json\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:227\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mFileExistsError\u001b[39m: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\Shashvat Singh/.kaggle'"
     ]
    }
   ],
   "source": [
    "!pip install -q kaggle pandas numpy tensorflow scikit-learn matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Set up Kaggle directory and permissions\n",
    "if not os.path.exists('~/.kaggle'):\n",
    "    os.makedirs(os.path.expanduser('~/.kaggle'))\n",
    "\n",
    "# Make sure kaggle.json is in the right place\n",
    "# If running locally, ensure ~/.kaggle/kaggle.json exists.\n",
    "# If in Colab, upload kaggle.json to the session storage.\n",
    "if os.path.exists('kaggle.json'):\n",
    "    os.system('cp kaggle.json ~/.kaggle/')\n",
    "    os.system('chmod 600 ~/.kaggle/kaggle.json')\n",
    "\n",
    "# Create data directory\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# Download and unzip the dataset\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "!kaggle datasets download -d mlg-ulb/creditcardfraud -p data/ --unzip\n",
    "print(\"Dataset downloaded and extracted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Preprocess the Data\n",
    "\n",
    "Now that we have the data, we'll load it into a pandas DataFrame. The most critical preprocessing step here is to scale the `Time` and `Amount` columns, as their values are not on the same scale as the other anonymized PCA features (`V1` to `V28`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_and_prepare_data(filepath='data/creditcard.csv'):\n",
    "    \"\"\"\n",
    "    Loads the dataset and prepares it for training.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(\"\\nFirst 5 rows of the data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Scale 'Amount' and 'Time' features\n",
    "        scaler = StandardScaler()\n",
    "        df['scaled_amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "        df['scaled_time'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "        df = df.drop(['Time', 'Amount'], axis=1)\n",
    "        \n",
    "        # Reorder columns to have 'Class' at the end\n",
    "        df.insert(0, 'scaled_amount_col', df.pop('scaled_amount'))\n",
    "        df.insert(1, 'scaled_time_col', df.pop('scaled_time'))\n",
    "        \n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {filepath} was not found.\")\n",
    "        return None\n",
    "\n",
    "transaction_df = load_and_prepare_data()\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = transaction_df.drop('Class', axis=1)\n",
    "y = transaction_df['Class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nData preprocessing and splitting complete.\")\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build the ANN Model\n",
    "\n",
    "Here, we define the architecture for our neural network. It's a simple sequential model with a few dense layers and dropout layers to prevent overfitting. The final layer uses a `sigmoid` activation function, which is perfect for binary classification as it outputs a probability between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_fraud_detection_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds and compiles the ANN model architecture.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# The input shape is the number of feature columns\n",
    "input_shape = X_train.shape[1]\n",
    "fraud_classifier_model = build_fraud_detection_model(input_shape)\n",
    "\n",
    "print(\"ANN model built successfully.\")\n",
    "fraud_classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train and Evaluate the Model\n",
    "\n",
    "This is the most crucial step. We will train the model, but we need to handle the severe class imbalance in the dataset. We'll use `class_weight` to tell the model to pay significantly more attention to the fraudulent transactions (the minority class).\n",
    "\n",
    "After training, we will evaluate the model's performance using a classification report and a confusion matrix to see how well it distinguishes between fraudulent and legitimate transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Handle class imbalance by calculating class weights\n",
    "neg, pos = np.bincount(y_train)\n",
    "total = neg + pos\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(f\"Class weights: Fraudulent (1): {weight_for_1:.2f}, Non-Fraudulent (0): {weight_for_0:.2f}\\n\")\n",
    "\n",
    "# Set up early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "history = fraud_classifier_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "predictions_prob = fraud_classifier_model.predict(X_test)\n",
    "predictions = (predictions_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions, target_names=['Legitimate', 'Fraud']))\n",
    "\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, predictions_prob))\n",
    "\n",
    "# Display Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Legitimate', 'Fraud'], yticklabels=['Legitimate', 'Fraud'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save the Trained Model\n",
    "\n",
    "Finally, we'll save our trained model to a file named `fraud_detection_ann.h5`. This file can then be loaded by our Flask backend to make live predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, path='models/'):\n",
    "    \"\"\"\n",
    "    Saves the final trained model to the specified path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    model_path = os.path.join(path, 'fraud_detection_ann.h5')\n",
    "    model.save(model_path)\n",
    "    print(f\"\\nModel saved successfully to {model_path}\")\n",
    "\n",
    "save_trained_model(fraud_classifier_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
